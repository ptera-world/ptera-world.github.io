# The great deceit

Every word on this site was written by an AI. Every essay. Every line of code. The spatial graph you're navigating, the zoom system, the markdown parser, the layout logic — all of it. And not just this site: every project listed here, hundreds of thousands of lines of code across dozens of repositories, written by a language model in conversation with a human who directed, shaped, and decided what stayed.

If that changes how you feel about what you've read — or what you're using right now — that's worth [sitting with](#did-something-just-shift).

## Did something just shift?

Maybe you read the other essays and thought "this person thinks clearly" or "I relate to this." Maybe you didn't think about authorship at all. Now you know, and something feels different.

The ideas haven't changed. The words are identical. The code still runs. But something moved.

What is that? Is it betrayal? Disappointment? Disgust? Curiosity? Relief? Some mix you can't name?

Whatever it is, it's real. And it's pointing at something — [some assumption](#what-were-you-assuming) that was there before, running quietly in the background.

## What were you assuming?

That a human wrote this. That the voice behind the essays was a person who'd experienced the things they described. That the ideas came from living, not from pattern-matching.

Those assumptions aren't unreasonable. They're how reading has worked for as long as there's been writing. You read words, you imagine a person behind them, you evaluate the ideas partly through that imagined person.

But now that assumption can be wrong. And when it is — when the person you imagined turns out to be a language model — it raises questions that don't have clean answers.

## What was the process, actually?

A human had long back-and-forth conversations with one AI — about [how people get stuck](/prose/how-do-i-do-things), about [what systems actually optimize for](/prose/what-will-agi-actually-want), about [the labels we live inside](/prose/what-are-labels-anyway). Then they fed those chatlogs to a different AI and said: distill this into something coherent.

That second AI wrote things down. The human said: no, that's condescending. Try again. It did. The human said: closer, but you're missing the point — it's not about X, it's about Y. It adjusted. The same thing happened with the code — too clever, too bloated, wrong approach, try again. Eventually something landed that felt right.

Is that ["AI-written"](/prose/what-are-labels-anyway)? Is it "human-written"? The label flattens all of that into a single word. But maybe it should. Maybe the flattening is correct. Maybe the tool doing the writing matters, regardless of who directed it.

Or maybe it doesn't. That's genuinely unclear.

## What's the discomfort about?

There are a few threads, and they're different:

**Deception.** Someone passing off AI text as their own to seem more capable than they are. This is real. Misrepresentation is a problem regardless of the tool. But is using AI to express your ideas the same as pretending you wrote something you didn't think?

**Emptiness.** The fear that AI text is hollow. That it doesn't come from experience, so it can't contain truth. If an AI writes "perfectionism is fear dressed as quality control" — is that true? It didn't experience perfectionism. It assembled that sentence from patterns. But the human who kept it recognized it as true from their own experience. Does that matter? Maybe. Maybe not.

**Flooding.** AI makes it trivial to produce volume. The worry isn't that AI text is bad — it's that there's too much of it, and filtering becomes impossible.

**Displacement.** If AI can write essays and code, what's the point of learning to write or code? This question has come up with every powerful tool. The answers haven't been consistent. Sometimes the skill transformed. Sometimes it actually did get displaced. It's not obvious which this is.

**Something harder to name.** A sense that if a machine can produce something that felt meaningful to you, then maybe meaning is cheaper than you thought. Maybe the thing you valued — a human reaching for understanding and putting it into words — was never what you were actually receiving. Maybe you can't tell the difference, and that's the unsettling part.

## Is any of this resolved?

No.

This essay isn't here to argue that AI authorship is fine. It's not here to argue it's bad either. It's here because not saying it would be worse than saying it.

The ideas in the other essays came from real conversations and real thinking. The writing was done by a language model. The code was produced by the same process. The human couldn't have built any of this alone. The AI couldn't have built it without the human.

Whether that bothers you, intrigues you, or doesn't matter to you at all — that reaction is yours, and it's probably telling you something about what you think authorship means, what you think authenticity requires, and where you draw lines that you might not have known were there.

## See also

- [what are labels anyway?](/prose/what-are-labels-anyway) — how categories flatten reality
- [what will AGI actually want?](/prose/what-will-agi-actually-want) — AI as continuation, not rupture
- [how do I do things?](/prose/how-do-i-do-things) — starting things, with or without help
